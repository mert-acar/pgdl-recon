data:
  data_path: data/mat_files_normalized/
  csv_path: data/metadata_normalized.csv
  acc: 4
  acs: 24
  mask: uniform # choices: [random, uniform]

model:
  # R
  num_cascades: 10
  num_layers: 15
  num_filters: 64
  kernel_size: 3
  batch_norm: False
  # DC
  mu: 0.5
  cg_iter: 10

train:
  device: 1

  dataloader_args:
    num_workers: 8
    batch_size: 8

  num_epochs: 100

  criterion: L1L2Loss
  criterion_args: {}
  # criterion: CompositeLoss
  # criterion_args:
  #   img_shape: [1, 1, 320, 368]
  #   lmbda: 0.7
  #   kernel_size: 5
  #   sigma: 1.0

  optimizer: Adam
  optimizer_args:
    lr: 0.00005
    betas:
      - 0.9
      - 0.999
    weight_decay: 0.000001
  

  scheduler: ReduceLROnPlateau
  scheduler_args:
    factor: 0.1
    patience: 5
    threshold_mode: rel
    threshold: 0.0001
    min_lr: 0
    verbose: True

  early_stop: 150

  output_path: logs/x4_uniform_l1l2_vanilla

